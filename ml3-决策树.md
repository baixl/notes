# 机器学习系列3:决策树
>
>date: 2018-05-13
>categories: 机器学习
>tags: 决策树
---
> 该系列将整理机器学习相关知识。这篇博客主要讨论:
> 决策树的基本思想、分类原理
> 决策树的分类
> 决策树的剪枝

## 1 引言
决策树是一种基本的分类回归方法。决策树模型是一种树形结构，构建决策树时根据特征空间对实例进行划分。决策树学习本质上是从训练数据集中归纳出一组分类规则，从另一个角度，决策树的学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率有无穷多个，决策树生成过程就是选择一个条件概率最大的模型。决策树学习通常包括3个步骤：1 特征选择，2 决策树生成，3 决策树修剪。根据特征选择算法的不同，决策树可以分类为ID3，C4.5和CART等。
<!--more-->
决策树的特征选择是从众多特征中，选择分类能力最好的特征作为分类标准。常见的特征选择算法有按信息增益（ID3）、按信息增益比（C4.5）、按基尼系数（CART）等。
按照不同的特征选择算法，决策树可以分为ID3，C4.5、CART等，其中CART即可以用于分类又可以用于回归。 
>在xgboost的算法介绍中，还定义了另外一种特征选择算法。


## 2 ID3
### 2.1 熵与信息增益
**熵：**
在信息论中，熵是随机变量的不确定性度量
X是一个取有限个离散值的随机变量，熵可以定义为:
$$\begin{equation}
Ent(x) = H(x)=-\sum_{i=1}^m p_i\log(p_i)
\end{equation}$$
通常，上面的对数以2为底或e为底，这是熵的单位为比特或纳特。熵H(x)只与x的分布有关，而与x的取值无关。当x只取两个值时，熵H(x)随p的变化如图：
![](http://7xnzwk.com1.z0.glb.clouddn.com/15259539362472.jpg)

从上图可以看出，当随机变量分布越散乱，熵越大，当随机变量分布越集中，比如p=1或p=0，熵越小。
从信息熵的角度，决策树学习就是找到一个特征，以该特征作为分裂依据，能够使分裂后的树的信息熵较少最多（即决策树越稳定），这个**信息熵的减少量**用信息增益来度量。  
假定离散属性a有V个可能的取值{a1,a2,..,av},若使用a来对D进行划分，则会产生 V 个分支节点，其中第v个分支节点包含了D中所有在属性a上取为av的样本，记为 Dv。则基于a属性划分后，获得的信息增益定义为:
$$\begin{equation} 
Gain(D,a) = Ent(D)-\sum_{v=1}^v \frac{|D^v|}{|D|}Ent(D^v) 
\end{equation}$$
一般而言信息，增益越大，意味着使用属性 a 划分后，所获得的样本纯度提升越大。因此，若使用信息增益为依据选择最优划分的属性，即选择信息增益最大的属性，作为划分依据。

以周志华《机器学习》上西瓜数据集为例，说明ID3算法流程：
![](http://7xnzwk.com1.z0.glb.clouddn.com/15261847680472.jpg)
上图为西瓜数据集，例子目标是判断一个瓜是否是好瓜（二分类）。
其中|Y|=2，总共有两类,则:
$$Ent(D) = -\sum_{k=1}^2 p_klog_2P_k=0.998$$
然后计算各个属性{色泽、根蒂、敲声、纹理、脐部、触感}中，以每个属性作为 划分依据所获得的信息增益。以色泽为例，可以划分为三个子集

D1={色泽=青绿}={1，4，6，10，13，17}，正例:p1=3/6 反例:p2=3/6 
D2={色泽=乌黑}={2，3，7，8，9，15},正例:p1=4/6 反例:p2=2/6 
D3={色泽=浅白}={5，11，12，14，16},正例:p1=1/5 反例:p2=4/5 
则色泽的信息增益为： 
![](http://7xnzwk.com1.z0.glb.clouddn.com/15261849015779.jpg)
类似方法，可依次计算出其他属性作为划分依据的信息增益： 
![](http://7xnzwk.com1.z0.glb.clouddn.com/15261849172400.jpg)
纹理信息增益最大，故第一轮以纹理作为划分依据。 
![](http://7xnzwk.com1.z0.glb.clouddn.com/15261849300962.jpg)
后续，在纹理划分的子集中，递归运用上述划分方法即可获得一颗决策树。
## 3 C4.5
C4.5是对ID3的改进。信息增益准对取值数目较多的属性有所偏好，当属性取值（种类，即某种属性可能有多个特征值）较多时，其信息增益会较大。为减少这种偏好带来的不利影响。C4.5算法做了一步改进，改而使用信息增益率作为属性划分标准。
信息增益率：
$$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
其中：
$$IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|} log_2 \frac{|D^v|}{|D|}$$
若属性a的取值较多，则对应的 IV(a)通常也会更大（相当于对特征值较多的属性做一个惩罚）。但是，增益率会对取值较少的属性有所偏好，**在 C4.5中的做法是:先从候选划分属性中找到信息增益高于平均水平的属性， 再从其中选择增益率最高的属性。**
## 4 CART
CART(分类回归树)，既可以用于分类，也可以用于回归。CART假设决策树是二叉树，内部节点分为左右(是、否)的两个分支（不管是分类还是回归，每个节点只有左右两支）。通过递归生成决策树。 
对回归问题使用平方误差最小化准则，对分类问题使用基尼指数最小化准则，进行特征选择。
### 4.1 分类树
CART树用于分类时，使用**基尼指数**来选择划分属性。假设样本有K个类别。样本点属于第 K 类的概率为 pk，则概率分布的基尼指数 定义为: 
$$Gini(y)=\sum_{k=1}^K p_k (1-p_k)=1-\sum_{k=1}^K {p_k}^2$$
数据集的纯度可以使用基尼指数来度量。基尼指数越小，数据集的纯度越高。

对于数据集D，基尼指数 Gini(D)表示集合D的不确定性，基尼指数 Gini(D,A)表示经 A=a 分割后，集合 D的不确定性。则在 CART 分类树中，选择一个是特征，该特征划分后集合 D 的基尼系数最小，则该特征作为最优划分特征。

如果样本集合 D 根据某特征 A 是否取值 a 被分割为 D1，D2 两部分：
$$D_1=\{ (x,y)\in D|A(x) =a \},  D_2 = D-D_1$$
则以特征A的划分后，集合 D 的基尼指数定义为: 
$$Gimi(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2) $$
### 4.2 回归树
一个回归树对应着输入空间的一个划分，以及在每个划分单元上的输出值。 和线性回归不同的是，回归树拟合的不是一条平滑曲线，回归树拟合的更像是一个锯齿状的曲线，即每一个叶子节点代表一个平均值，通过决策树，预测值最终取的是分到叶子节点的平均值 
假设输入空间划分为 M 个单元:R1，R2，....，RM,并且在每个单元 Rm 上有一个固定的输出值 cm,则回归树可以表示为: 
$$f(x)=\sum_{m=1}^M c_m I(x \in R_m)$$
当输入空间划分确定后，可以使用平方误差作为损失函数：
$$Cost(x) = \sum_{x \in R_m} (y-f(x))^2$$
那如何选择属性和值作为划分标准呢？这里使用启发式规则，选择第j个属性x作为划分特征，和其对应的取值s作为划分取值，将样本划分成如下两个部分：
$$R_1(j,s)=\{x|x^j \leq s \},R_2(j,s)=\{x|x^j > s \}$$
然后通过如下公式求最优划分特征和取值：
$$ \min_{j,s}[\min_{c1} \sum_{xi \in R1}(y_i - c_1)^2 + \min_{c2} \sum_{xi \in R2}(y_i - c_2)^2]$$
其中：
$c_1 = avg(y_i|R_1(j,s)) = \frac{1}{N_1}\sum_{xi \in R_1}y_i$
$c_2 = avg(y_i|R_2(j,s)) = \frac{1}{N_2}\sum_{xi \in R_2}y_i$

找到最优切分点后，递归按照上述方法进行切分，每次寻求最优切分点的依据是平方误差最小。
## 5 决策树的剪枝
在决策树学习中，为了尽可能正确分类训练样本，结点划分过程不断重复，有时会造成过拟合（分支过多）。决策树的减枝是避免决策树过拟合的常用方法。 
常用的减枝包**预减枝**和**后减枝**： 

* 预剪枝：预剪枝是指在决策树生成过程中，对每个节点划分前进行估计。若当前节点的划分不能带来决策树泛化能力（通过交叉验证评估）的提升，则停止划分并将当前节点标记为叶子节点。 
* 后剪枝：先从训练集生成一个颗完整的决策树，然后自底向上对非叶子节点进行搜索，若将该节点对应的子树替换为叶子节点，并且能带来泛化能力的提升，则将该子树替换成叶子节点。

预剪枝使得决策树很多分支都没有展开，这不仅可以降低过拟合，同时还显著减少了决策树训练的时间开销。
后剪枝通常比预剪枝保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛化性能优于预剪枝。



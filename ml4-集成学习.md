# 机器学习系列4:集成学习
---
> 该系列将整理机器学习相关知识。这篇博客主要讨论:
> 1 什么是集成学习？
> 2 集成学习的总类？
> 3 随机森林
> 4 boosting框架
> 这篇博客紧接着上一篇：决策树，在集成学习中用到较多决策树的知识。

## 1 什么是集成学习？
俗话说"三个臭皮匠，顶个诸葛亮"。在机器学习中，集成学习也是类似这种思想，意思是多个模型结合起来的作用好于单个模型。这里需要先说一下强可学习和弱科学系的概念
<!--more-->
* 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的； 
* 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。 

![](http://7xnzwk.com1.z0.glb.clouddn.com/15262048321808.jpg)
如上图，集成学习是将多个学习器进行结合，常可以获得比单个学习器效果更好的泛化能力。每一个个体学习器称为弱学习器（基学习器）。在一般的经验中，将多个好坏不等的学习器掺杂到一起，那么通常的结果会比最坏的好一些，比最好的差一些。集成学习可以把多个学习器结合起来，可以获得比最好的学习器更好的性能。 举个例子，假设有三个基学习器h1、h3，使用投票法进行集成，通过下图的对比可以看出，为获得更好的集成性能，基学习器要“好而不同”。 即基学习器不能太差，并且要具有多样性。 
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262049885991.jpg)

根据个体学习器的生成方式，集成学习大致分为两类：

* 个体学习器之间不存在强依赖关系，可以并行生成。代表：Bagging和随机森林。
* 个体学习器之间存在强依赖关系，必须序列化生成。代表：Boosting(AdaBoost,提升树)

## 2 Bagging与随机森林
### 2.1 Bagging思想
随机森林中的单个学习器是决策树。由于决策树容易产生过拟合，随机森林采用多个决策树进行投票或者平均的机制，来改善单个决策树的泛化误差。
在上面介绍了集成学习中，单个学习器应该好而不同，个体学习器应该尽可能独立。在现实中，无法得到完全独立的决策树，但通常应该使基学习器应该尽可能不同。
**Bagging：** 它基于自助采样法(bootstrap sampling)，在原始数据集中多次采样，用每个采样的子集构造决策树。在采样后，大概有63.2%的数据出现在新样本中：

假定样本空间大小为m，先随机取出一个样本放入采样集，再把该样本放回重复采样，共采样m次，这样会得到m个采样样本（有放回采样），样本在m次采样中，始终不被采样到的概率为$(1- \frac{1}{m})^m$,  

$$ \lim_{m \rightarrow +\infty} (1- \frac{1}{m})^m = \frac{1}{e} \approx  0.368$$

在采样得到的m个样本下，训练决策树：

* 对分类问题，可以使用投票法进行集成
* 对回归问题，可以使用平均法进行集成

自助采样法还有一个好处，每个学习器只使用了63.2%的原始训练数据，剩下的36.8%可以用于测试
### 2.2 随机森林
随机森林在以决策树为几学习器构建Bagging集成的基础上，进一步在训练过程中引入了随机属性选择。
具体来说，传统决策树在进行划分时，会在当前的节点属性集合中选择一个最优属性。而在RF中，对基决树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集(k=log2d,d是当前节点的待选属性个数),然后再从这个子集中选择一个最优属性用于划分。

随机森林中的两个随机：

* 自助采样（随机有放回采样）
* 树生成过程中随机选择属性子集
 
上述两个随机过程，保证了决策树的多样性，即使不对随机森林中的决策树进行减枝，也不容易over-fitting.
## 3 Adaboost
### 3.1 AdaBoost算法原理
AdaBoost是Boosting框架下的一个基本方法。Adaboost的工作机制：先从初始训练集训练一个基学习器，再根据基学习器的表现，对训练样本分布进行调整，前一个基分类器分类错误的样本会在下一轮训练得到加强，加权后的样本**全体**会在下一轮再次训练一个基本分类器。同时，每一轮都加入一个若分类器。

具体来说，Adboost算法分为3步：[https://blog.csdn.net/v_july_v/article/details/40718799](https://blog.csdn.net/v_july_v/article/details/40718799)

1. 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
2. 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
3. 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。


设训练数据集$T={(x_1,y_1),(x_2,y_2),(x_3,y_3)...(x_n,y_n)}, y \in \{-1, +1\}$,

* 步骤1：初始化训练数据的权重：
$$D_1=(w_{11},w_{12},...w_{1n}), w_{1i}=\frac{1}{N}, i=1,2...N$$

* 步骤2：进行多轮迭代，用$m=1,2...M$表示迭代次数
  - 2.1 在权值分布$D_m$上训练数据，得到基本分类器：
  $$G_m(x):X \rightarrow \{-1,+1\}$$
  - 2.2计算$G_m(x)$在训练数据集上的误差率：
  $$e_m = P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i) \neq y_i)=\sum_{G_m(x_i)\neq y_i} w_{mi}$$
  这里$w_{mi}$表示第m轮中第i实例的权值。
  - 2.3 计算$G_m(x)$的系数：

  $$\alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}$$
  $𝛼_m$表示$𝐺_m(x)$ 在最终分类器中的重要性。可以看出$𝛼_m$随着$𝑒_m$的减小而增大，即分类误差率越小的基分类器，在最终分类器中的作用越大。
  
  - 2.4 更新训练数据集的权值分布：
  ![](http://7xnzwk.com1.z0.glb.clouddn.com/15262086508545.jpg)
* 步骤3：构建基本分类器的线性组合
$$f(x) = \sum_{m=1}^M\alpha_mG_m(x)$$
得到最终分类器为：
$$G(x) = sign(f(x)) = sign(\sum_{m=1}^M\alpha_mG_m(x))$$

### 3.2 AdaBoost实例
>来自统计学习方法

![](http://7xnzwk.com1.z0.glb.clouddn.com/15262088544190.jpg)
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262088694108.jpg)
## 4 提升树
### 4.1 提升树算法原理
>来自《统计学习方法》

提升树是以分类树或者回归树为基本分类器的提升方法。提升树采用`加法模型`与`前向分步算法`，以`决策树`为基函数。

* 对分类问题，决策树是二叉分类树
* 对回归问题决策树是二叉回归树

基本分类器为x<v或者x>v,可以看做是一个根节点直接两个叶子节点的简单决策树，即决策树桩。
针对不同问题的提升树，使用的损失函数不同：

* 使用平方损失函数的回归问题
* 使用指数损失函数的分类问题
* 使用一般损失函数的一般决策问题

**对于二分类问题**：提升树算法只需将AdaBoost算法中 的基本分类器限制为二类分类树即可，这时的提升树算 法是AdaBoost算法的特殊情况。

**下面重点讨论回归问题：**

提升树模型可以表示为决策树的假发模型：$f_M(x) = \sum_{m=1}^MT(x;\theta_m)$,其中$T(x;\theta_m)$表示决策树，$\theta_m$表示决策树的参数，M为决策树的个数。

提升树使用前向分部算法求解。首先假设初始提升树为：$f_0(x) =0$, 第m步的模型为：
$$f_m(x)=f_{m-1}(x)+T(x;\theta_m)$$
此时，$f_{m-1}$为当前模型，通过经验风险最小化求下一颗决策树$\theta_m$。则对于回归问题的前向分部算法为：
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262186115253.jpg)
对于回归问题的提升树，每一次迭代拟合的是上一步的残差。
### 4.2 提升树示例
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262187174876.jpg)
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262187251174.jpg)
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262187356610.jpg)
$$L(y,f_1(x)) = \sum_{i=1}^{10}(y_i -f_1(x_i))^2  = 1.93$$

第 2 步:求 $T_2(x)$,方法同第 1 步，拟合数据为第 1 步计算的残差，可以得到模型:
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262188316875.jpg)
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262188397517.jpg)
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262188686246.jpg)

## 6 方差偏差分析
![](http://7xnzwk.com1.z0.glb.clouddn.com/15262190929732.jpg)

从偏差方差角度：**Bagging侧重于降低方差。Boosting主要关注降低偏差** 
Bagging中两个操作 Booststrap（随机自采样）和随机选择k个特征，每一轮都加入了随机扰动（样本和特征），可以降低过拟合的可能。 
Boosting是减少偏差：通过提升每一个基学习器的泛化能力，降低偏差。




